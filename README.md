# Randomised-Attack
I've created a basic technique called Random Masking on a CIFAR-10 dataset, which includes randomly choosing pixels and setting them to zero or some other random value. The attacker iteratively perturbs the input data in small steps with the goal of finding the smallest possible perturbation that causes the model to misclassify the input; the idea is to disrupt the underlying patterns in the input data and force the model to learn more robust and resilient features than in PGD. To maximise the loss function while remaining within a fixed distance threshold, PGD iteratively distorts the input image. In contrast to random masking, which only requires picking and applying a random matrix from a given set to each channel of an image, PGD attacks can take a long time depending on a variety of factors such as the number of iterations, the size of the step size, and the distance threshold.
